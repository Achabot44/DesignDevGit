{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Dracula.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Dunwich.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Frank.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Hyde.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Opera.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Paw.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Usher.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Vampyre.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Wendigo.txt', 'C:\\\\Users\\\\Home PC Dell\\\\Desktop\\\\Dr. Salter Design & Development\\\\WeekTwelveTexts\\\\Willows.txt']\n"
     ]
    }
   ],
   "source": [
    "#Collect and import ten documents\n",
    "#I tried to segment this off so it was not pulling such a long part of the destination, but I got errors every other way I sliced it.\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "documents = []\n",
    "path = 'C:\\\\Users\\Home PC Dell\\Desktop\\Dr. Salter Design & Development\\WeekTwelveTexts\\\\'\n",
    "\n",
    "filenames=sorted([os.path.join(path, fn) for fn in os.listdir(path)])\n",
    "print(len(filenames)) # count files in corpus\n",
    "print(filenames[:10]) # print names of 1st ten files in corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as text\n",
    "\n",
    "vectorizer=text.CountVectorizer(input='filename', stop_words=\"english\", min_df=1)\n",
    "dtm=vectorizer.fit_transform(filenames).toarray() # defines document term matrix\n",
    "\n",
    "vocab=np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of document-term matrix: (10, 18116). Number of tokens 189791\n"
     ]
    }
   ],
   "source": [
    "# Using the topic modeling code as a starter, build a topic model of the documents\n",
    "print(f'Shape of document-term matrix: {dtm.shape}. '\n",
    "      f'Number of tokens {dtm.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the topic modeling code as a starter, build a topic model of the documents\n",
    "import sklearn.decomposition as decomposition\n",
    "model = decomposition.LatentDirichletAllocation(\n",
    "    n_components=100, learning_method='online', random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the topic modeling code as a starter, build a topic model of the documents\n",
    "document_topic_distributions = model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the topic modeling code as a starter, build a topic model of the documents\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "# (# topics, # vocabulary)\n",
    "assert model.components_.shape == (100, len(vocabulary))\n",
    "# (# documents, # topics)\n",
    "assert document_topic_distributions.shape == (dtm.shape[0], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                00       000        01        02        03        04  \\\n",
      "Topic 0   0.240440  0.201934  0.195902  0.185536  0.243303  0.191426   \n",
      "Topic 1   0.224398  0.216225  0.228078  0.215852  0.232036  0.192563   \n",
      "Topic 2   0.183774  0.203513  0.207648  0.211981  0.182050  0.180348   \n",
      "Topic 3   0.176281  0.210842  0.206563  0.213356  0.184010  0.190914   \n",
      "Topic 4   0.180408  0.225164  0.202078  0.222010  0.195744  0.192582   \n",
      "...            ...       ...       ...       ...       ...       ...   \n",
      "Topic 95  0.208073  0.211070  0.234611  0.183877  0.205050  0.203618   \n",
      "Topic 96  0.186383  0.191290  0.249839  0.236537  0.220568  0.233022   \n",
      "Topic 97  0.210253  0.210102  0.160942  0.209011  0.194179  0.213187   \n",
      "Topic 98  0.238109  0.194913  0.185927  0.189436  0.238293  0.192360   \n",
      "Topic 99  0.187463  0.232779  0.203234  0.212698  0.174711  0.190650   \n",
      "\n",
      "                05        10       100     10000  ...       zip    zipped  \\\n",
      "Topic 0   0.236945  0.169275  0.227390  0.187183  ...  0.221985  0.237676   \n",
      "Topic 1   0.222349  0.213153  0.187411  0.216412  ...  0.176087  0.203828   \n",
      "Topic 2   0.166842  0.224715  0.180893  0.214090  ...  0.247119  0.209691   \n",
      "Topic 3   0.189368  0.234819  0.200909  0.220560  ...  0.218139  0.234956   \n",
      "Topic 4   0.213019  0.183163  0.217746  0.196154  ...  0.256999  0.160847   \n",
      "...            ...       ...       ...       ...  ...       ...       ...   \n",
      "Topic 95  0.184063  0.212738  0.188351  0.215974  ...  0.217080  0.233132   \n",
      "Topic 96  0.201397  0.227794  0.228302  0.196922  ...  0.200818  0.217627   \n",
      "Topic 97  0.214141  0.197028  0.200238  0.191032  ...  0.204053  0.206970   \n",
      "Topic 98  0.219686  0.203344  0.190191  0.190109  ...  0.173210  0.209096   \n",
      "Topic 99  0.178221  0.229582  0.224966  0.201871  ...  0.226847  0.237655   \n",
      "\n",
      "              zone       zoo  zoölogical  zoöphagous  zoöphagy    zurich  \\\n",
      "Topic 0   0.174601  0.217961    0.182991    0.210725  0.203811  0.213133   \n",
      "Topic 1   0.227497  0.205404    0.221506    0.219295  0.167689  0.183513   \n",
      "Topic 2   0.206530  0.188272    0.179252    0.213118  0.222212  0.167781   \n",
      "Topic 3   0.194974  0.201826    0.214088    0.229652  0.203301  0.201686   \n",
      "Topic 4   0.163898  0.209839    0.256220    0.210615  0.192431  0.235142   \n",
      "...            ...       ...         ...         ...       ...       ...   \n",
      "Topic 95  0.188090  0.209894    0.177311    0.180709  0.223804  0.212887   \n",
      "Topic 96  0.222801  0.189435    0.201359    0.208754  0.219977  0.187948   \n",
      "Topic 97  0.195323  0.220852    0.175503    0.239346  0.173646  0.178570   \n",
      "Topic 98  0.179291  0.207013    0.218026    0.187524  0.237449  0.209301   \n",
      "Topic 99  0.177363  0.198316    0.217116    0.200421  0.211135  0.207562   \n",
      "\n",
      "                æt      ætat  \n",
      "Topic 0   0.221987  0.195277  \n",
      "Topic 1   0.190767  0.202546  \n",
      "Topic 2   0.193848  0.191478  \n",
      "Topic 3   0.230203  0.168015  \n",
      "Topic 4   0.202384  0.167762  \n",
      "...            ...       ...  \n",
      "Topic 95  0.217013  0.197675  \n",
      "Topic 96  0.209775  0.191993  \n",
      "Topic 97  0.182159  0.210639  \n",
      "Topic 98  0.231009  0.201092  \n",
      "Topic 99  0.173049  0.206021  \n",
      "\n",
      "[100 rows x 18116 columns]\n"
     ]
    }
   ],
   "source": [
    "#Using the topic modeling code as a starter, build a topic model of the documents\n",
    "topic_names = [f'Topic {k}' for k in range(100)]\n",
    "topic_word_distributions = pd.DataFrame(\n",
    "    model.components_, columns=vocabulary, index=topic_names)\n",
    "print(topic_word_distributions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
