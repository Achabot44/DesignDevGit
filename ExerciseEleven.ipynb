{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home PC Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "#Import at least ten documents from files, using the OS module and any others relevant to process the text.\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import at least ten documents from files, using the OS module and any others relevant to process the text.\n",
    "#I have changed and recultivated numerous texts for this section. I am committing to the ones I have now as I went into each one and eliminated \n",
    "#everything that was not directly from the original text of the stories being used.\n",
    "textdir = 'C:\\\\Users\\Home PC Dell\\Desktop\\\\Dr. Salter Design & Development\\\\WeekElevenTexts\\\\'\n",
    "os.chdir(textdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each filename in the directory you listed...\n",
    "#I am spending more time in note pad, my file directory and guttenburg trying to solve this problem than I am in python.\n",
    "#I am calling it. I have six texts that work and the last eight that I have tried have caused my code to crash. I know that we are supposed to\n",
    "#Have 10, but five hours in I am going to run with the six that work.\n",
    "for filename in os.listdir(textdir):\n",
    "    #If the filename ends with .txt...\n",
    "    if filename.endswith('.txt'):\n",
    "        #Create an output name that adds '-nouns' to the filename\n",
    "        outname = filename.replace('.txt','-nouns.txt')\n",
    "        #Open the file\n",
    "        with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "            #Open the output file\n",
    "            with open(outname, 'w') as out:\n",
    "                #Read the text from the file\n",
    "                text = f.read()\n",
    "                #Split the text into a list of sentences\n",
    "                sentences = nltk.sent_tokenize(text)\n",
    "                #For each sentence in the list of sentences...\n",
    "                for sentence in sentences:\n",
    "                    #For each word and each part-of-speech tag that you get\n",
    "                    #When NLTK tokenizes the sentence (splitting words from punctuation, etc.)\n",
    "                    for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "                        #If the part-of-speech is noun\n",
    "                        if (pos == 'NN' or pos == 'NNS'):\n",
    "                       \n",
    "                            #Write the word (which should be a noun) to the output file\n",
    "                            out.write(word)\n",
    "                            #Write a space so the words don't smush together\n",
    "                            out.write(' ')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
