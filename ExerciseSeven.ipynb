{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Import at least three documents you would like to compare.\r\n",
    "import urllib.request, urllib.error, urllib.parse\r\n",
    "\r\n",
    "url = 'https://www.gutenberg.org/cache/epub/4018/pg4018.txt'\r\n",
    "\r\n",
    "response = urllib.request.urlopen(url)\r\n",
    "webContent = response.read()\r\n",
    "\r\n",
    "f = open('Ozaki.txt', 'wb')\r\n",
    "f.write(webContent)\r\n",
    "f.close()\r\n",
    "\r\n",
    "print(webContent[0:1000])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of Japanese Fairy Tales, by Yei Theodora Ozaki\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Japanese Fairy Tales\\r\\n\\r\\nAuthor: Yei Theodora Ozaki\\r\\n\\r\\nPosting Date: June 4, 2009 [EBook #4018]\\r\\nRelease Date: May, 2003\\r\\nFirst Posted: October 11, 2001\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK JAPANESE FAIRY TALES ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by Charles Franks, Greg Weeks and the Online\\r\\nDistributed Proofreading Team. HTML version by Al Haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nJAPANESE FAIRY TALES\\r\\n\\r\\nCOMPILED BY\\r\\n\\r\\nYEI THEODORA OZAKI\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProfusely Illustrated by Japanese Artists\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTO\\r\\n\\r\\nELEANOR MARION-CRAWFORD.\\r\\n\\r\\nI DEDICATE THIS BOOK TO YOU AND TO THE SWEET CHILD-FRIENDSHIP THAT YOU\\r\\nGAVE ME IN THE DAYS SPENT WITH YOU BY THE'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import urllib.request, urllib.error, urllib.parse\r\n",
    "\r\n",
    "url = 'https://www.gutenberg.org/cache/epub/28/pg28.txt'\r\n",
    "\r\n",
    "response = urllib.request.urlopen(url)\r\n",
    "webContent = response.read()\r\n",
    "\r\n",
    "f = open('Aesop.txt', 'wb')\r\n",
    "f.write(webContent)\r\n",
    "f.close()\r\n",
    "\r\n",
    "print(webContent[0:1000])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b\"\\xef\\xbb\\xbfThe Project Gutenberg EBook of Aesop's Fables, by Aesop\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Aesop's Fables\\r\\n\\r\\nAuthor: Aesop\\r\\n\\r\\nPosting Date: December 18, 2011 [EBook #28]\\r\\nRelease Date: March 8, 1992\\r\\nLast Updated: March 15, 2002\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK AESOP'S FABLES ***\\r\\n\\r\\n\\r\\n**********************************************************************\\r\\nEBOOK (#30) WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A\\r\\nTIME WHEN PROOFING METHODS AND TOOLS WERE NOT WELL DEVELOPED. THERE IS\\r\\nAN IMPROVED EDITION OF THIS TITLE WHICH MAY BE VIEWED AT EBOOK #18732\\r\\n**********************************************************************\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n                       AESOP'S FABLES (82 Fables)\\r\\n\\r\\n\\r\\nFrom The PaperLess Rea\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import urllib.request, urllib.error, urllib.parse\r\n",
    "\r\n",
    "url = 'https://www.gutenberg.org/cache/epub/537/pg537.txt'\r\n",
    "\r\n",
    "response = urllib.request.urlopen(url)\r\n",
    "webContent = response.read()\r\n",
    "\r\n",
    "f = open('Doyle.txt', 'wb')\r\n",
    "f.write(webContent)\r\n",
    "f.close()\r\n",
    "print(webContent[0:1000])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b\"\\xef\\xbb\\xbfProject Gutenberg's Tales of Terror and Mystery, by Arthur Conan Doyle\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Tales of Terror and Mystery\\r\\n\\r\\nAuthor: Arthur Conan Doyle\\r\\n\\r\\nPosting Date: October 10, 2008 [EBook #537]\\r\\nRelease Date: May, 1996\\r\\n[Last updated: December 8, 2011]\\r\\n\\r\\nLanguage: English\\r\\n\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK TALES OF TERROR AND MYSTERY ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by Charles Keller.  HTML version by Al Haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTales of Terror and Mystery\\r\\n\\r\\n\\r\\nBy\\r\\n\\r\\nSir Arthur Conan Doyle\\r\\n\\r\\n\\r\\n\\r\\nContents\\r\\n\\r\\nTales of Terror\\r\\n\\r\\n  The Horror of the Heights\\r\\n  The Leather Funnel\\r\\n  The New Catacomb\\r\\n  The Case of Lady Sannox\\r\\n  The Terror of Blue John Gap\\r\\n  The Brazilian Cat\\r\\n\\r\\n\\r\\nTales of Mystery\\r\\n\\r\\n  The Lost Special\\r\\n  The Beetle-Hunt\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Preprocess the text and create a tokenized corpus from the text of the imported documents.\r\n",
    "import nltk\r\n",
    "import nltk.tokenize\r\n",
    "\r\n",
    "# download the most recent punkt package\r\n",
    "nltk.download('punkt', quiet=True)\r\n",
    "\r\n",
    "corpus = [\"Ozaki.txt\",\"Aesop.txt\",\"Doyle.txt\"]\r\n",
    "titles = [\"Japanese Fairy Tales\",\"Aesop's Fables\",\"Tales of Terror and Mystery\"]\r\n",
    "documents = []\r\n",
    "for url in corpus:\r\n",
    "    f = open(url, encoding='utf-8')\r\n",
    "    text = f.read()\r\n",
    "    documents.append(text)\r\n",
    "print(documents[1][0:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Project Gutenberg EBook of Aesop's Fables, by Aesop\n",
      "\n",
      "This eBook is for the use of anyone anywhe\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "#Preprocessing: checking to see if a string is not a punctuation marker\r\n",
    "import re\r\n",
    "\r\n",
    "PUNCT_RE = re.compile(r'[^\\w\\s]+$')\r\n",
    "\r\n",
    "def is_punct(string):\r\n",
    "    \r\n",
    "    return PUNCT_RE.match(string) is not None\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "##Preprocessing: putting text into lower case tokens\r\n",
    "def preprocess_text(text, language, lowercase=True):\r\n",
    "   \r\n",
    "    if lowercase:\r\n",
    "        text = text.lower()\r\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language=language)\r\n",
    "    tokens = [token for token in tokens if not is_punct(token)]\r\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#Preprocessing: this demonstrates that the strings have been tokenized and put in lower case \r\n",
    "tokenized = []\r\n",
    "for text in documents:\r\n",
    "    tokenized.append(preprocess_text(text, \"english\"))\r\n",
    "\r\n",
    "print(tokenized[0][11])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ozaki\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#Preprocessing: here we are creating a list of unique words that will be placed in alpha order and make sure the text is ready to be turned into a term-matrix\r\n",
    "def extract_vocabulary(tokenized_corpus, min_count=1, max_count=float('inf')):\r\n",
    "   \r\n",
    "    vocabulary = collections.Counter()\r\n",
    "    for document in tokenized_corpus:\r\n",
    "        vocabulary.update(document)\r\n",
    "    vocabulary = {word for word, count in vocabulary.items()\r\n",
    "                  if count >= min_count and count <= max_count}\r\n",
    "    return sorted(vocabulary)\r\n",
    "import collections\r\n",
    "vocabulary = extract_vocabulary(tokenized, min_count=2)\r\n",
    "print(vocabulary[0:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"'and\", \"'as-is\", \"'crack\", \"'devil\", \"'he\", \"'is\", \"'it\", \"'ll\", \"'m\", \"'s\", \"'the\", \"'ve\", \"'yes\", \"'you\", '//gutenberg.org/license', '//pglaf.org', '//pglaf.org/donate', '//pglaf.org/fundraising', '//www.gutenberg.org', '//www.pglaf.org', '1', '1.', '1.a', '1.b', '1.c', '1.d', '1.e', '1.e.1', '1.e.2', '1.e.3', '1.e.4', '1.e.5', '1.e.6', '1.e.7', '1.e.8', '1.e.9', '1.f', '1.f.1', '1.f.2', '1.f.3', '1.f.4', '1.f.5', '1.f.6', '15', '1500', '1890', '18th', '1908.', '2', '2.', '20', '2001', '2011', '21st', '3', '3.', '30', '3rd', '4', '4.', '4557', '4th', '5,000', '5.', '50', '501', '596-1887', '60', '64-6221541.', '713', '77b', '7th', '8', '801', '809', '84116', '90', '99712.', 'a', 'abandon', 'abandoned', 'abide', 'ability', 'ablaze', 'able', 'abnormal', 'abode', 'abominable', 'about', 'above', 'abroad', 'absence', 'absent', 'absolute', 'absolutely', 'absurd', 'absurdly', 'abundance', 'accept', 'accepted']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#Create a document term matrix to enable comparative textual analysis across the full set of documents\r\n",
    "#Chart at least one comparison between the documents, using word frequency to map the text\r\n",
    "def corpus2dtm(tokenized_corpus, vocabulary):\r\n",
    "    \r\n",
    "    document_term_matrix = []\r\n",
    "    for document in tokenized_corpus:\r\n",
    "        document_counts = collections.Counter(document)\r\n",
    "        row = [document_counts[word] for word in vocabulary]\r\n",
    "        document_term_matrix.append(row)\r\n",
    "    return document_term_matrix\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "document_term_matrix = np.array(corpus2dtm(tokenized, vocabulary))\r\n",
    "old_id = vocabulary.index('old')\r\n",
    "young_id = vocabulary.index('young')\r\n",
    "\r\n",
    "old_counts = document_term_matrix[:, old_id]\r\n",
    "young_counts = document_term_matrix[:, young_id]\r\n",
    "print(\"Old: \" + str(old_counts))\r\n",
    "print(\"Young: \" + str(young_counts))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Old: [401  24  68]\n",
      "Young: [52 17 46]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}